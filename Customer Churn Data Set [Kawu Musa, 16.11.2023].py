# -*- coding: utf-8 -*-
"""Customer Churn Data Set [Kawu Musa "Mufasa", 16.11.2023].ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iCrLLqRqoUvHuU-WVBc0I97ZPq6bX9Sw

# Machine Learning Project - Customer Churn Data Set [By Mufasa; (Currently Learning from Udemy)]

Link: https://www.kaggle.com/datasets/adammaus/predicting-churn-for-bank-customers?select=Churn_Modelling.csv

# Part 1 - Data Pre-processing

## Step 1 - Importing Libraries and Data set
"""

import numpy as np
import pandas as pd
import seaborn as sbn
import matplotlib.pyplot as plt

#reading csv file using pandas and saving into a variable called 'data'
data = pd.read_csv('/content/Churn_Modelling.csv')

#previewing the top 5 rows in the dataset
data.head()

"""## Step 2 - Data Exploration"""

#Finding out the number of rows and columns in the dataset
data.shape

#Obtaining basic information about the dataset
data.info()

#Finding the column that has categorical (Non-numerical) data.
data.select_dtypes(include='object').columns

#Finding the number of column(s) that have/has categorical (Non-numerical) data.
len(data.select_dtypes(include='object').columns)

#Finding the column(s) that have / has numerical data.
data.select_dtypes(include=['float64','int64']).columns

#Finding the number of column(s) that have / has numerical data.
len(data.select_dtypes(include=['float64','int64']).columns)

#Summarising statistical information about the columns in the dataset.
data.describe()

"""## Step 3 - Dealing with Missing values"""

#Are there null values in the dataset?
data.isnull().values.any()

#The number of null values in the dataset?
data.isnull().values.sum()

"""##Step 4 - Encoding Categorical Data"""

#Show the columns with non-numerical variables
data.select_dtypes(include='object').columns

#Remove columns that won't have any effect on prediction
data = data.drop(columns=['RowNumber','CustomerId','Surname'])

#Show unique categories in Geography column

data['Geography'].unique()

#Show unique categories in Gender column

data['Gender'].unique()

#Group dataset by Geography - Note the German-Balance

data.groupby('Geography').mean()

#Group dataset by Gender

data.groupby('Gender').mean()

#Apply one hot encoding to categorical variables

data = pd.get_dummies(data=data,drop_first=True)

data.head()

#Obtaining number of rows where member didn't exit
(data.Exited ==0).sum()

#Obtaining number of rows where member exited
(data.Exited ==1).sum()

"""##Step 5 - Correlation Matrix and Heatmap"""

#Dataset without dependent variable
data_1 = data.drop(columns='Exited')

#Correlation between 'exited column' and other independent columns
data_1.corrwith(data['Exited']).plot.bar(figsize = (16,9), title='Correlated with Exited',rot=45,grid=True)

correlation = data.corr()

#Plotting a heatmap that shows correaltion betweeen the columns
plt.figure(figsize=(16,9))
sbn.heatmap(correlation,annot=True)

"""##Step 6 - Splitting the Dataset"""

#Dropping the 'exited' column from the dataset
features_matrix = data.drop(columns='Exited')

#A matrix of the dependent column

target_matrix = data['Exited']

#Splitting the dataset into training and test data (70% - 30%)
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(features_matrix, target_matrix, test_size=0.3, random_state=0)

#Seeing the number of rows and columns in training data (for independent variables)

X_train.shape

#Seeing the number of rows and columns in training data (for dependent variables)

y_train.shape

#Seeing the number of rows and columns in test data (for independent variables)


X_test.shape

#Seeing the number of rows and columns in test data (for dependent variables)

y_test.shape

"""##Step 7 - Feature Scaling"""

from sklearn.preprocessing import StandardScaler

#Instantiating Standard scaling function
standard_scaler = StandardScaler()

#Scaling training and test data (independent variables)

X_train = standard_scaler.fit_transform(X_train)
X_test = standard_scaler.transform(X_test)

X_train

X_test

"""#Part 2 - Building the Model

## 1) Logistic Regression
"""

from sklearn.linear_model import LogisticRegression
classifier_logisticregression = LogisticRegression(random_state = 0)
classifier_logisticregression.fit(X_train,y_train)

y_predicted = classifier_logisticregression.predict(X_test)

from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score

acc = accuracy_score(y_test,y_predicted)
f1 = f1_score(y_test,y_predicted)
precision = precision_score(y_test,y_predicted)
recall = recall_score(y_test,y_predicted)

results_logr = pd.DataFrame([['Logistic Regression', acc, f1, precision, recall]],columns = ['Model','Accuracy','F1 Score', 'Precision', 'Recall score'])

results_logr

confusion_m = confusion_matrix(y_test,y_predicted)
print(confusion_m)

"""###Cross Validation"""

from sklearn.model_selection import cross_val_score
accuracy_lr = cross_val_score(estimator=classifier_logisticregression,X=X_train,y=y_train,cv=10)

print("Accuracy is {:.2f}%".format(accuracy_lr.mean()*100))
print("SD is {:.2f}%".format(accuracy_lr.std()*100))

"""##2) Random Forest"""

from sklearn.ensemble import RandomForestClassifier
Classifier_randomforest = RandomForestClassifier(random_state = 0)
Classifier_randomforest.fit(X_train,y_train)

y_predicted_rf = Classifier_randomforest.predict(X_test)

from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score

acc_rf = accuracy_score(y_test,y_predicted_rf)
f1_rf = f1_score(y_test,y_predicted_rf)
precision_rf = precision_score(y_test,y_predicted_rf)
recall_rf = recall_score(y_test,y_predicted_rf)

results_RandomForest = pd.DataFrame([['RandomForest', acc_rf, f1_rf, precision_rf, recall_rf]],columns = ['Model','Accuracy','F1 Score', 'Precision', 'Recall score'])

results_RandomForest

confusion_rf = confusion_matrix(y_test,y_predicted_rf)
print(confusion_rf)

results_logr.append(results_RandomForest,ignore_index=True)

"""###Cross Validation"""

from sklearn.model_selection import cross_val_score
accuracy_RandomForest = cross_val_score(estimator=Classifier_randomforest,X=X_train,y=y_train,cv=10)

print("Accuracy is {:.2f}%".format(accuracy_RandomForest.mean()*100))
print("SD is {:.2f}%".format(accuracy_RandomForest.std()*100))

